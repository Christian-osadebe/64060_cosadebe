---
title: "Hierarchical Clustering"
author: "64060_cosadebe"
date: "2025-11-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning =  FALSE)
```

## Loading Necessary Libraries
```{r}
library(stats)
library(cluster)
library(dplyr)
library(caret)
library(mclust)

```

## Data Loading and Preparation
```{r}
# Load the Cereals.csv dataset
Cereals <- read.csv("Cereals.csv")

# Remove missing values
Cereals <- na.omit(Cereals)

# Update row names with Cereal labels
rownames(Cereals) <- Cereals[,1]

# Identify categorical variables
Cereals_char_cols <- names(Cereals)[sapply(Cereals, is.character)]

# Retain only numeric variables
Cereals_num <- Cereals %>%
  select(- any_of(Cereals_char_cols))

```

## Q1. Hierarchical Clustering with Euclidean Distance and Complete Linkage on the full dataset
```{r}
# Normalize the Cereals.csv numerical dataset
Cereals_norm <- scale(Cereals_num)

# Compute the dissimilarity matrix
dist_matrix <- dist(Cereals_norm, method = "euclidean")

# Perform Hierarchical clustering using "complete" distance metric  
hc_results <- hclust(dist_matrix, method = "complete")

# plot the obtained dendrogram
plot(hc_results, cex = 0.4, hang = -1)

```

## Comparing AGNES Clustering Results Using Single, Complete, Average, and Ward Linkage Methods
```{r}
# Apply AGNES clustering
hc_single <-  agnes(Cereals_norm, method = "single")
hc_complete <-  agnes(Cereals_norm, method = "complet")
hc_average <-  agnes(Cereals_norm, method = "average")
hc_ward <-  agnes(Cereals_norm, method = "ward")


# Compare Agglomerative Coefficient Across Linkage Methods
cat("Agglomerative Coefficient Comparison:\n",
    "Single Linkage:  ", round(hc_single$ac, 2), "\n",
    "Complete Linkage:", round(hc_complete$ac, 2), "\n",
    "Average Linkage: ", round(hc_average$ac, 2), "\n",
    "Ward Linkage:    ", round(hc_ward$ac, 2), "\n\n",
    "Best Method:     Ward (Highest AC =", round(hc_ward$ac, 2),")")

```

Interpretation:

Based on the agglomerative coefficient values, the best clustering method is the Ward linkage, with an agglomerative coefficient of 0.9. Higher values indicate more cohesive and well-separated clusters, making Ward the most effective method in this comparison.

### Q2. Choosing the number of clusters, k.

To determine the optimal number of clusters in AGNES, I used Silhouette Analysis, which offers a statistically robust measure of cluster cohesion and separation — superior to heuristic methods like dendrogram inspection or the elbow method (Shmueli et al., 2020). I tested only k=2 to k=10, selecting the value with the highest average silhouette width to ensure well-separated clusters.
```{r}
# Compute silhouette widths for k = 2 to 10
avg_sil <- sapply(2:10, function(k) {
  sil <- silhouette(cutree(hc_ward, k), dist_matrix)
  return(mean(sil[, 3]))
  })

# Plot average silhouette widths to identify optimal k
plot(2:10, avg_sil,
     type = "b",
     xlab = "Number of clusters",
     ylab = "Average silhouette width")

```

Decision: Based on the silhouette plot, the number of clusters with the highest average silhouette width is k = 10, which is therefore selected as the optimal clustering solution.

### Visualizing AGNES clustering structure with dendrogram and cluster boundaries
```{r}
pltree(hc_ward, cex = 0.4, hang = -1, main = "Dendrogram of agnes")
rect.hclust(hc_ward, k = 10, border = 1:10)
```

## Q3. Evaluation of Structure and Stability of the Clusters

### Step 1: Partitioning the Dataset & Applying AGNES Clustering on Partition A
```{r}
# Partition the Cereals_num dataset
set.seed(123)
train_index <- createDataPartition(Cereals_num$calories, p = 0.5, list = FALSE) 
df_A <- Cereals[train_index, ]
df_B <- Cereals[-train_index, ]

# Normalize df_A and df_B excluding categorical cols
df_A_norm <- scale(df_A[, -c(1:3)])   
df_B_norm <- scale(df_B[, -c(1:3)])

# Apply AGNES clustering on Partition A
hc_A <- agnes(df_A_norm, method = "ward")

# Visualize AGNES clustering structure with dendrogram and cluster boundaries with k=7
pltree(hc_A, cex = 0.4, hang = -1, main = "Dendrogram of agnes")
rect.hclust(hc_A, k = 7, border = 1:7)
```

### Step 2: Extracting Cluster Labels and Computing Centroids for Partition A
```{r}
# Extract cluster labels from AGNES dendrogram
clusters_A <- cutree(hc_A, k = 7)

# Compute centroids of clusters in partition  A
centroids_A <- as.data.frame(df_A_norm) %>%
  mutate(cluster = clusters_A) %>%
  group_by(cluster) %>%
  summarise(across(where(is.numeric), mean)) %>%
  select(-cluster)    # remove the cluster label col

```

### Step 3: Assigning Partition B Records to Nearest Centroids from Partition A
```{r}
assign_to_centroid <- function(x, centroids) {
  # compute the squared Euclidean distance
  dists <- apply(centroids, 1, function(c) sum((x - c)^2))
  # return index (cluster label) of the closest centroid
  return(which.min(dists))}

# Assign partition B records to cluster labels
assigned_B <- apply(df_B_norm, 1, assign_to_centroid, centroids = centroids_A)

```

### Step 4:  Extracting true B labels from clustering the full dataset in Q1
```{r}
# Derive cluster labels from clustering on the full dataset
clusters_full <- cutree(hc_ward, k = 7)

# Extract the true cluster labels corresponding to Partition B records
true_B <- clusters_full[-train_index]

```

### Step 5: Assessing Cluster Consistency Between Partition B Assignments and Full Dataset Labels
```{r}
# Evaluate consistency using Adjusted Rand Index (ARI)
ari <- adjustedRandIndex(assigned_B, true_B) 

cat("ARI:", round(ari,2))
```

Cluster stability was evaluated using the Adjusted Rand Index (ARI; Hubert & Arabie, 1985). The ARI ranges from -1 to 1, where 1: perfect agreement (clusters are identical), 0: agreement no better than random chance, 
and < 0: Worse than random.

The comparison between Partition B assignments and the full-data clustering produced an ARI of 0.70, indicating high stability. This suggests that the clusters are generally consistent, with only minor variation, and that the AGNES solution is reasonably reproducible.

## Q4. Clustering “Healthy Cereals” for School Cafeterias

Cereal datasets typically include nutritional variables such as calories, protein, fat, etc., and these variables are measured on different scales. To ensure fair comparison, the data should be normalized before clustering; otherwise, the results will be biased toward variables with larger numeric ranges. If normalization is not applied, all variables must at least be measured on the same scale and in the same units.

When normalization is omitted, clustering should be restricted to variables measured on a common scale, or limited to the most relevant variables so that the analysis reflects meaningful health criteria rather than being distorted by scale differences. In such cases, it is important to acknowledge that the results are biased toward high‑range variables and interpret the clustering accordingly.


Reference:

- Shmueli, G., Bruce, P. C., Yahav, I., Patel, N. R., & Lichtendahl, K. C. (2020). Data mining for business analytics: Concepts, techniques, and applications in Python. Wiley.

- Hubert, L., & Arabie, P. (1985). Comparing partitions. Journal of Classification, 2(1), 193–218. https://doi.org/10.1007/BF01908075






































